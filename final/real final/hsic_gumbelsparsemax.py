# -*- coding: utf-8 -*-
"""HSIC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-b4FsWLLbCuyyKltuTb6onGIJGS8UP4W
"""

import torch.nn.functional as F
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sparsemax import Sparsemax
import math



# Neural Network with Gumbel-Softmax
class HSICNetGumbelSparsemax(nn.Module):
    def __init__(self, input_dim, hidden_dim1, hidden_dim2, sigma_init_X, sigma_init_Y, num_sampling):
        super(HSICNetGumbelSparsemax, self).__init__()
        self.input_dim = input_dim
        self. hidden_dim1 = hidden_dim1
        self. hidden_dim2 = hidden_dim2
        self.num_sampling = num_sampling
        self.fc1 = nn.Linear(input_dim, hidden_dim1)
        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)
        self.fc3 = nn.Linear(hidden_dim2, input_dim)
        self.relu = nn.ReLU()
        
        self.sigmas = nn.Parameter(sigma_init_X)
        self.sigma_y = nn.Parameter(sigma_init_Y)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        logits = self.fc3(x)

        # Apply Gumbel-Softmax sampling with 5 samples and temperature of 1
        #number of important features is equal to the number of sampling
        # self.importance_weights = self.gumbel_sparsemax_sampling(logits, temperature=10, num_sampling=1)
        self.importance_weights = self.gumbel_sparsemax_sampling(logits, temperature=10, num_sampling = self.num_sampling)

        return self.importance_weights, self.sigmas, self.sigma_y


    
    # Gumbel-Sparsemax Sampling
    def gumbel_sparsemax_sampling(self, logits, temperature=0.1, num_sampling=5):
        """
        Perform Gumbel-Sparsemax sampling multiple times and return the max value for each feature.
        """
        # np.random.seed(0)

        batch_size, d = logits.size()

        # Expand logits to [BATCH_SIZE, 1, d] for sampling multiple times
        logits_expanded = logits.unsqueeze(1)  # [BATCH_SIZE, 1, d]

        # Uniform random noise [BATCH_SIZE, k, d]
        uniform = torch.rand(batch_size, num_sampling, d, device=logits.device)

        # Sample Gumbel noise and add to logits
        gumbel = -torch.log(-torch.log(uniform + 1e-20) + 1e-20)
        noisy_logits = (gumbel + logits_expanded) / temperature

        # changed softmax to Sparsemax
        sparsemax = Sparsemax(dim=-1)
        samples = sparsemax(noisy_logits)

        # Take the maximum over the k samples
        samples = torch.max(samples, dim=1)[0]

        return samples

    def anova_kernel(self, X1, X2, s, sigmas):
        """
        ANOVA kernel incorporating feature importance (s) and per-feature sigma.
        Args:
        - X1, X2: Input tensors of shape (num_samples, num_features)
        - s: Feature importance weights (output of the Gumbel-Softmax)
        - sigmas: Trainable sigmas for each feature
        Returns:
        - Kernel matrix for inputs X1 and X2
        """
        ##*******
        # sigmas = 0.1 *torch.ones(X1.size(1))
        ##*******
        prod = torch.ones((X1.size(0), X2.size(0)))
        for i in range(X1.size(1)):  # iterate over features
            dists = (X1[:, i].unsqueeze(1) - X2[:, i].unsqueeze(0)) ** 2
            prod *= (1 + s[:, i].unsqueeze(1) * torch.exp(-dists / (2 * sigmas[i]**2)))
        return prod



# Modified RBF Kernel for label y with learnable sigma
    def rbf_kernel_y(self, y1, y2, sigma_y):
        """
        RBF Kernel for the label y, incorporating a trainable sigma_y.
        """

        ###******************
        # sigma_y = 0.1 
        ###******************
        dists = (y1.unsqueeze(1) - y2.unsqueeze(0)) ** 2
        return torch.exp(-dists / (2 * sigma_y**2))

    def hsic_loss_adaptive(self, X, y, s, sigmas, sigma_y):
        X_kernel = self.anova_kernel(X, X, s, sigmas)
        y_kernel = self.rbf_kernel_y(y, y, sigma_y)

        # Centering the kernels
        n = X.size(0)
        H = torch.eye(n).to(X.device) - (1 / n) * torch.ones(n, n).to(X.device)

        X_centered = H @ X_kernel #@ H
        y_centered = H @ y_kernel #@ H

        # HSIC value with proper scaling
        hsic_value = torch.trace(X_centered @ y_centered) / (n - 1) ** 2
        return hsic_value

    # Training function
    def train_model(self, X, y, num_epochs=300, lr=1e-3, BATCH_SIZE = 1000):
        
        optimizer = optim.Adam(self.parameters(), lr=lr)
        train_dataset = TensorDataset(X, y)
        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

        for epoch in range(num_epochs):
            self.train()
            for inputs, outputs in train_loader:
                optimizer.zero_grad()

                s, sigmas, sigma_y = self(inputs)  # importance weights from Gumbel-Sparsemax
                loss = -self.hsic_loss_adaptive(inputs, outputs, s, sigmas, sigma_y)  # Minimize negative HSIC

                loss.backward()
                optimizer.step()

                if (epoch + 1) % 50 == 0:
                    print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}")

     
    def instancewise_shapley_value(self, X_train, y_train, X_samples, y_samples, sigmas, sigma_y, weights):
        n, d = X_train.shape
        n_samples = X_samples.shape[0]

       
        sv = torch.zeros(n_samples, d)
        hsic_values = torch.zeros(n_samples)

        # Loop through each sample in X_samples
        for idx in range(n_samples):
            x = X_samples[idx].unsqueeze(1)
            y = y_samples[idx]

            Ks = torch.zeros(n, d)
            anova_k = torch.ones(n)

            # Compute kernels for the current sample
            for i in range(d):
                dists = (x[i] - X_train[:, i]) ** 2
                k = weights[idx, i] * torch.exp(-dists / (2 * sigmas[i]**2))
                anova_k *= (1 + k)
                Ks[:, i] = k
            anova_k -= 1

            k_y = torch.exp(- (y - y_train) ** 2 / (2 * sigma_y**2))

            k_x_avg = torch.mean(anova_k)
            k_y_avg = torch.mean(k_y)

            # Compute Shapley values for the current sample
            for i in range(d):
                sv[idx, i], k_x_tilde = self.instancewise_sv_dim(Ks, k_y, k_x_avg, k_y_avg, i, n)
            
            # Compute HSIC for the current sample
            hsic_values[idx] = (anova_k - k_x_avg) @ (k_y - k_y_avg)

        return sv, hsic_values


    def instancewise_sv_dim(self, Ks, k_y, k_x_avg, k_y_avg, dim, num_samples):
        dp = torch.zeros(self.input_dim, self.input_dim, num_samples)
        n, d = Ks.shape

        Ks_copy = Ks.clone().detach()
        Ks_copy[:, 0] = Ks[:, dim]
        Ks_copy[:, dim] = Ks[:, 0]

        sum_current = torch.zeros((n,))

        # Fill the first order dp (base case)
        for j in range(d):
            dp[0, j, :] = Ks_copy[:, j]
            sum_current += Ks_copy[:, j]

        for i in range(1, self. input_dim):
            temp_sum = torch.zeros((num_samples,))
            for j in range(self. input_dim):
                # Subtract the previous contribution of this feature when moving to the next order
                sum_current -= dp[i - 1, j, :]

                dp[i, j, :] = (i/(i+1)) * Ks_copy[:, j] * sum_current
                temp_sum += dp[i, j, :]

            sum_current = temp_sum

        k_x_tilde = torch.sum(dp[:, 0, :], axis=0)
        # print(k_x_tilde)

        X_centered = k_x_tilde - k_x_avg
        y_centered = k_y - k_y_avg

        # HSIC value with proper scaling
        sv_i = (X_centered @ y_centered)

        return sv_i, k_x_tilde

    def global_shapley_value(self, X_train, y_train, sigmas, sigma_y, weights):
        n, d = X_train.shape
        Ks = torch.zeros(d, n, n)
        anova_k = torch.ones(n, n)

        for i in range(X_train.size(1)):  # iterate over features
            dists = (X_train[:, i].unsqueeze(1) - X_train[:, i].unsqueeze(0)) ** 2
            k = (weights[:, i].unsqueeze(1) *
                torch.exp(-dists / (2 * sigmas[i]**2)))
            anova_k *= (1 + k)
            Ks[i, :, :] = k
        anova_k -= 1

        dists = (y_train.unsqueeze(1) - y_train.unsqueeze(0)) ** 2
        k_y = torch.exp(-dists / (2 * sigma_y**2))

        H = torch.eye(n).to(X_train.device)  # - (1 / n) * torch.ones(n, n).to(X_train.device)

        # We define inclusive and noninclusive weights for value functions that inlcude/not-include the the corresponding feature
        inclusive_weights = torch.zeros(d, 1)
        noninclusive_weights = torch.zeros(d, 1)

        for i in range(d):
            inclusive_weights[i] = math.factorial(i) * math.factorial(d - i - 1)
            if i < d-1:
                noninclusive_weights[i] = math.factorial(
                    i+1) * math.factorial(d - (i + 1) - 1)
        inclusive_weights /= math.factorial(d)
        noninclusive_weights /= math.factorial(d)

        sv = torch.zeros(d, 1)
        for i in range(d):
            sv[i], k_tilde, dp = self.global_sv_dim(Ks, k_y, H, i)

        hsic = torch.trace(H @ anova_k @ H @ k_y) / (n - 1) ** 2
        return sv, hsic

    def global_sv_dim(self, Ks, k_y, H, dim):
        d, n = Ks.shape[0], Ks.shape[1]

        dp = torch.zeros(d, d, n, n)

        Ks_copy = Ks.clone().detach()
        Ks_copy[0, :, :] = Ks[dim, :, :]
        Ks_copy[dim, :, :] = Ks[0, :, :]

        sum_current = torch.zeros((n, n))

        # Fill the first order dp (base case)
        for j in range(d):
            dp[0, j, :, :] = Ks_copy[j, :, :]
            sum_current += Ks_copy[j, :, :]

        for i in range(1, d):
            temp_sum = torch.zeros((n, n))
            for j in range(d-i):
                # Subtract the previous contribution of this feature when moving to the next order
                sum_current -= dp[i - 1, j, :, :]

                dp[i, j, :, :] = (i / (i+1)) * Ks_copy[j, :, :] * sum_current
                temp_sum += dp[i, j, :, :]

            sum_current = temp_sum

        k_tilde = torch.sum(dp[:, 0, :, :], axis=0)

        sv_i = torch.trace(H @ k_tilde @ H @ k_y) / (n - 1) ** 2

        return sv_i, k_tilde, dp
    
    def predict(self, X):
        """
       
        Returns:
            torch.Tensor: Predicted importance weights for each feature.
        """
        self.eval()  # Set the model to evaluation mode
        with torch.no_grad():  # Disable gradient computation
            importance_weights, sigmas, sigma_y = self(X)  # Forward pass
        
        return importance_weights, sigmas, sigma_y


def initialize_sigma_median_heuristic(X):
        """
        X: Tensor of shape (n_samples, n_features)
        Returns: Initial sigma values for each feature using the median heuristic
        """
        n, d = X.size()
        sigma_init = torch.zeros(d)

        # Calculate median of pairwise distances for each feature
        for i in range(d):
            feature_values = X[:, i].unsqueeze(1)
            pairwise_dists = torch.cdist(feature_values, feature_values, p=2).squeeze()
            sigma_init[i] = torch.median(pairwise_dists)

        return sigma_init

def initialize_sigma_y_median_heuristic(Y):
    """
    Y: Tensor of shape (n_samples, 1) for the outputs
    Returns: Initial sigma_Y using the median heuristic
    """
    # Ensure Y is 2D (n_samples, 1)
    if len(Y.shape) == 1:
        Y = Y.unsqueeze(1)

    # Compute pairwise distances for outputs
    pairwise_dists = torch.cdist(Y, Y, p=2).squeeze()
    sigma_Y_init = torch.median(pairwise_dists)

    return sigma_Y_init

